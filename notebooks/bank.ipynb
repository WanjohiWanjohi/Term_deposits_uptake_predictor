{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WanjohiWanjohi/Term_deposits_uptake_predictor/blob/master/bank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soly_X1c7XE9",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.utils import resample\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6uF5b7L99Rxc",
        "colab": {}
      },
      "source": [
        "#get the file \n",
        "#df =  pd.read_csv('bank-additional/bank-additional/bank-additional-full.csv' , sep=\";\" , engine='python')\n",
        "df = pd.read_csv('/content/drive/My Drive/bank-additional-full.csv' , sep=\";\")\n",
        "df.to_pickle(\"./bankcsv.pkl\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGG8C2w6oJod",
        "colab_type": "text"
      },
      "source": [
        " The duration skews the analysis as it is not known before a call is made and should therefore be dropped before we feed this data to our model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y3xB-3WYC9Nb",
        "colab": {}
      },
      "source": [
        "df.drop('duration' , axis=1 , inplace=True)\n",
        "df.info() , df.describe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mdxCVOcrDJGk"
      },
      "source": [
        "From the information , we can see we have no null entries in the dataframe \n",
        "Hence , we can go forward and process our data by:\n",
        "1. Encoding Categorical Data\n",
        "2. Scale\n",
        "3. Class Balance in target variable\n",
        "5. Aggregating and Transformation\n",
        "6. Dimension reduction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX9zjCzKNUSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_categorical(dataframe , columns):\n",
        "   df = dataframe[columns]\n",
        "   converted_df = pd.get_dummies(df)\n",
        "   res = pd.concat([df, converted_df], axis=1)\n",
        "   return res\n",
        "\n",
        "def get_column_counts(df ):\n",
        "  \"\"\"\n",
        "  Get count of unique items in the dataframe column\n",
        "  \"\"\"\n",
        "  for c in df.columns:\n",
        "    print(c)\n",
        "    print(df[c].value_counts())\n",
        "\n",
        "def scale_columns(df , column):\n",
        "  \"\"\"\"\n",
        "  Scale columns using MinMax scaler\n",
        "  \"\"\"\n",
        "  col = df[column]\n",
        "  scaler = MinMaxScaler() \n",
        "  num2 = scaler.fit_transform(col)\n",
        "  num2 = pd.DataFrame(num2, columns = col.columns)\n",
        "  return num2\n",
        "\n",
        "def upsample(df , majority_col, minority_col):\n",
        "  \"\"\"\"\n",
        "  Perform upsampling by resampling while replacing\n",
        "  \"\"\"\n",
        "  # we seperate the classes\n",
        "  majority_class = df[majority_col]\n",
        "  minority_class = df[minority_col]\n",
        "  #Upsample minority class\n",
        "  df_minority_upsampled = resample(minority_class, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=36548,    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "  # Combine majority class with upsampled minority class\n",
        "  df_upsampled = pd.concat([majority_class, df_minority_upsampled])\n",
        "  return df_upsampled\n",
        "\n",
        "def convert_dates(df , column , format):\n",
        "  \"\"\"\n",
        "  convert date and months to numerical format\n",
        "\n",
        "  \"\"\"\n",
        "  months = []\n",
        "  day_of_week = []\n",
        "  #convert column items to strings\n",
        "  \n",
        "    #format months\n",
        "  if format == \"%b\":\n",
        "    for index, row in df[column].items():\n",
        "      datetime_object = datetime.datetime.strptime(row , \"%b\")\n",
        "      month_number = datetime_object.month\n",
        "      months.append(month_number)\n",
        "    return months\n",
        "    ## format days\n",
        "  elif format == \"%a\":\n",
        "    for index, row in df[column].items():\n",
        "      datetime_object = datetime.datetime.strptime(row , \"%a\")\n",
        "      day_number = datetime_object.weekday()\n",
        "      day_of_week.append(day_number)\n",
        "    return day_of_week\n",
        "def get_correlation_map(correlation_matrix):\n",
        "  \"\"\"\n",
        "  return a heatmap of the correlation\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(20,5))\n",
        "  return sns.heatmap(correlation_matrix)\n",
        "\n",
        "\n",
        "def get_target_sample(df , column , sample_number):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  sample = df[column].sample(n=int(sample_number), random_state=1)\n",
        "  return sample\n",
        "\n",
        "def pca_reduction(df , components):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  feat_cols = df.columns.tolist()\n",
        "  pca = PCA(n_components=components).fit(df)\n",
        "  var_exp = pca.explained_variance_ratio_\n",
        "  #apply the dimensionality reduction\n",
        "  transformed = pca.transform(df)\n",
        "  \n",
        "  return transformed\n",
        "\n",
        "def logistic_reg(X,y):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.1,random_state=1)\n",
        "  lr = LogisticRegressionCV(random_state=0).fit(X_train, y_train)\n",
        "  prediction=lr.predict(X_test)\n",
        "  return prediction , y_train\n",
        "\n",
        "def get_roc_auc_score(true_y , predicted_y , avg = \"macro\"):\n",
        "  \"\"\"\"\n",
        "  \"\"\"\n",
        "  score = roc_auc_score(true_y , predicted_y ,average=avg )\n",
        "  return score\n",
        "\n",
        "def XGBoost(X , y):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # convert the dataset into a Dmatrix that gives it  performance and efficiency gains.\n",
        "  data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
        "  xg_reg = xgb.XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n",
        "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
        "  xg_reg.fit(X_train,y_train)\n",
        "  preds = xg_reg.predict(X_test)\n",
        "  params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
        "                'max_depth': 5, 'alpha': 10}\n",
        "  cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
        "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"auc\", as_pandas=True, seed=123)\n",
        "  return preds , cv_results\n",
        "\n",
        "def multi_layer_percep(X , y):\n",
        "  \"\"\"\n",
        "  \n",
        "  \"\"\"\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
        "  clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
        "  predict = clf.predict(X_test)\n",
        "  return y_train , predict\n"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM5x56ZhMbT5",
        "colab_type": "text"
      },
      "source": [
        "#### Encode categorical data \n",
        "We use the get_dummies() function availed by pandas\n",
        "\n",
        "0 indicates non existent while 1 indicates existent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCP2B_45DTmr",
        "colab": {}
      },
      "source": [
        "categorical = ['job' , 'marital' , 'education' , 'contact' , 'housing' , 'default' , 'loan' , 'poutcome']\n",
        "target = ['y' ]\n",
        "binned = ['pdays']\n",
        "dates = ['month' , 'day_of_week']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ0ys6eBRGx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_pickle(\"./bankcsv.pkl\")\n",
        "converted_df = convert_categorical(df , categorical)\n",
        "get_column_counts(converted_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ZY3TC1fbdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "month = convert_dates(df , dates[0]  , '%b')\n",
        "day = convert_dates(df , dates[1] , '%a')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okTWX9dRRUPi",
        "colab_type": "text"
      },
      "source": [
        "#### Scale\n",
        "We scale the pdays , employee numbers and age columns column to have a restricted range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erT6ejyhoJpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled = scale_columns(df , binned)\n",
        "scaled_emp_no = scale_columns(df , ['nr.employed'])\n",
        "scaled_age = scale_columns(df , ['age'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBVgewl7UKiI",
        "colab_type": "text"
      },
      "source": [
        "#### Class Balancing\n",
        "Imbalanced classes put “accuracy” out of business. This is a surprisingly common problem in machine learning (specifically in classification), occurring in datasets with a disproportionate ratio of observations in each class as is the case in our dataset with the target variable\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkoLZXROUKL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "b31aaa6f-0a98-403c-cd91-4f298f41f783"
      },
      "source": [
        "#check existence of class imbalance\n",
        "get_column_counts(df[target])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y\n",
            "no     36548\n",
            "yes     4640\n",
            "Name: y, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvIAFZT8VDGr",
        "colab_type": "text"
      },
      "source": [
        "With a population of 41,188 , the yes population makes up only 11% of the population\n",
        "We handle this by upsampling the minority class(the yes)\n",
        "\n",
        "1. First, we'll separate observations from each class into different DataFrames.\n",
        "2. Next, we'll resample the minority class with replacement, setting the number of samples to match that of the majority class.\n",
        "3. Finally, we'll combine the up-sampled minority class DataFrame with the original majority class DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMJIwpAwWqS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#   we start by encoding the content of the target column\n",
        "target_df = convert_categorical(df , target)\n",
        "\n",
        "#   use upsample function\n",
        "upsampled = upsample(target_df ,'y_no' , 'y_yes' )\n",
        "\n",
        "target_column = pd.DataFrame(upsampled , columns=[\"deposit\"])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGvf0Exxgaoc",
        "colab_type": "text"
      },
      "source": [
        "#### Transform and Aggregate\n",
        "*First , let us bring it all together into one dataframe*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tppf5Oa_gpZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine the encoded , scaled and datetime formatted columns\n",
        "## drop the categorical columns\n",
        "converted_df.drop(categorical , axis=1 , inplace=True)\n",
        "converted_df\n",
        "\n",
        "# bring in the campaign columns and the age column\n",
        "converted_df['age'] = scaled_age\n",
        "converted_df['campaign'] = df['campaign']\n",
        "converted_df['previous'] = df['previous']\n",
        "\n",
        "#bring in the derived month column\n",
        "converted_df['month'] = month\n",
        "\n",
        "#bring in the scaled employee numbers and number of days that pass before a customer is contacted\n",
        "converted_df['pdays'] = scaled\n",
        "converted_df['employee_no'] = scaled_emp_no\n",
        "\n",
        "\n",
        "converted_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oszeAEBNmbUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate total number of contacts made with user\n",
        "sum_column = converted_df['previous'] + converted_df['campaign']\n",
        "converted_df['total_contacts'] = sum_column"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k80HON1Soo8c",
        "colab_type": "text"
      },
      "source": [
        "#### Reduce the dimensions\n",
        "45 columns is a large number ; additionally some of them report the same result\n",
        "Dimensionality reduction can be done in two different ways:\n",
        "\n",
        "  - By only keeping the most relevant variables from the original dataset (this technique is called feature selection)\n",
        "  - By finding a smaller set of new variables, each being a combination of the input variables, containing basically the same information as the input variables (this technique is called dimensionality reduction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CBjUl00qvoS",
        "colab_type": "text"
      },
      "source": [
        "###### Low Variance filtering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHL19la_qfaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "variance = converted_df.var()\n",
        "variance\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtTVvW5QrXPX",
        "colab_type": "text"
      },
      "source": [
        "We can safely drop the day column since it varies never\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSfx4qIIrepU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converted_df.drop('day' , axis=1 , inplace=True)\n",
        "converted_df.astype('float64').dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7XwXyJjr1S8",
        "colab_type": "text"
      },
      "source": [
        "#### Principal Compnent Analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwfoZJaG4VJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformed_features = pca_reduction(converted_df , 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQpE1B7BI73T",
        "colab_type": "text"
      },
      "source": [
        "### Modeling\n",
        "We use three main model approaches :\n",
        "  - Logistic regression\n",
        "  - XGBoost\n",
        "  - Multi Layer Perceptron\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSaxgAcHuXc",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fAwXr2DoJp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = transformed_features  \n",
        "y = get_target_sample(target_column , 'deposit' , 41188  )"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkoYy1BsrSlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred, true = logistic_reg(X , y)\n",
        "#get evaluation of mode\n",
        "\n",
        "#save model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxhwpqEt6rA0",
        "colab_type": "text"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56peSp1Q1T1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "67b5f4a6-1aae-416c-af2e-ebd11f261bc6"
      },
      "source": [
        "pred , cv_results = XGBoost(X, y)\n",
        "#get evaluation of model\n",
        "cv_results\n",
        "#save model"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train-auc-mean</th>\n",
              "      <th>train-auc-std</th>\n",
              "      <th>test-auc-mean</th>\n",
              "      <th>test-auc-std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.523309</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.499027</td>\n",
              "      <td>0.00436</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
              "0        0.523309       0.000815       0.499027       0.00436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvEZTQHZNdjL",
        "colab_type": "text"
      },
      "source": [
        "#### Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv-KDSm1Pfau",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "5640d5e7-fc12-4951-9370-43e29a14eb62"
      },
      "source": [
        "multi_layer_percep(X , y)\n"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1928     0\n",
              " 31617    0\n",
              " 9584     1\n",
              " 12035    1\n",
              " 34658    1\n",
              "         ..\n",
              " 30865    0\n",
              " 1327     1\n",
              " 10278    0\n",
              " 35859    1\n",
              " 10469    1\n",
              " Name: deposit, Length: 30891, dtype: uint8,\n",
              " array([1, 1, 1, ..., 1, 0, 0], dtype=uint8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms-doEtrOPQ0",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "http://deeplearning.net/tutorial/mlp.html"
      ]
    }
  ]
}